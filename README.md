# UFront
Unified MLIR Frontend for Deep Learning 

## How to reproduce the results
### Option 1 (recommended):

Experiencing UFront on Kaggle (for model compilation, performance comparison, ImageNet inference, accuracy validation, etc.)

Run the anonymous UFront online tests in Kaggle using the links below, **be sure to login** to use full functionality and free GPU (T4x2) resources.

https://www.kaggle.com/code/anomyuser/ufront-test/

https://www.kaggle.com/code/anomyuser/test-torch

https://www.kaggle.com/code/anomyuser/test-tf

**Note**: the results on Kaggle may slightly different from the paper reported because of different CPU and GPU used.

**Important:** Access GPU at no cost or turn on an internet connection. Need to **login** and **Get phone verified** in Kaggle.

**The Internet Connection** in the Kaggle notebook need to be **turned on** to allow package download.

### Option 2 (contains performance comparison on: CPU, RTX 3070, A100):
Execute provided jupyter notebooks in the examples folder, be sure to install dependencies (see following guideline):
```sh
test_tf.ipynb

test_torch.ipynb

test_accuracy.ipynb

test_tf_A100.ipynb (results on **NVidia A100**)

test_torch_A100.ipynb (results on **NVidia A100**)

```

### Option 3 (suitable for debug):
Execute python scripts, you may install corresponding dependencies manually.

```sh
python examples/torch_test.py
python examples/bert_test.py
python examples/lstm_test.py
python examples/keras_test.py
python examples/onnx_test.py
```


### Option 4

ImageNet inference with UFront locally

1) Download ImageNet validation set (about 2GB, named "imagenet1kvalid") from kaggle.com and extract it to a local folder

2) In the example/test_accuracy.ipynb (jupyter notebook), change the root path to the parent folder of "imagenet1kvalid", execute all notebook cells (depend on your local Python version, you may install different UFront packages).
   

## Examples' dependencies
### Install pre-build ufront package
In Ubuntu 20.04 or 22.04, download corresponding ufront package in the release folder and install.

Install any of the following packages according to your default Python version.
```sh
pip install ufront-0.1.1-cp37-cp37m-manylinux_2_28_x86_64.whl #for Python3.7

pip install ufront-0.1.1-cp38-cp38-manylinux_2_28_x86_64.whl #for Python3.8

pip install ufront-0.1.1-cp39-cp39-manylinux_2_28_x86_64.whl #for Python3.9

pip install ufront-0.1.1-cp310-cp310-manylinux_2_28_x86_64.whl #for Python3.10

pip install ufront-0.1.1-cp311-cp311-manylinux_2_28_x86_64.whl #for Python3.11
```

### Install Execution Backend (IREE)
Recommended stable IREE

```sh
!pip install iree-compiler==20230524.529 iree-runtime==20230524.529 
```
For older Python like Python3.7 you may install previous iree because recent iree release does not provide Python 3.7 support:
```sh
!pip install iree-compiler==20230330.474 -f https://github.com/iree-org/iree/releases/download/candidate-20230330.474/iree_compiler-20230330.474-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl

!pip iree-runtime==20230330.474 -f https://github.com/iree-org/iree/releases/download/candidate-20230330.474/iree_runtime-20230330.474-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```

For CUDA 12+, you may install newer IREE (see trouble shootings).

## Trouble shootings

### 1. CUDA RuntimeError

If you experiencing the following error, you need to upgrade NVidia Driver and CUDA; or you can lower IREE to a lower version, e.g., 20230330.474 (howover, the results may slightly different from reported). The PTX code generated by recent IREE is not compatible with old NVidia Driver.

RuntimeError: Error creating vm context with modules:main_checkout/runtime/src/iree/hal/drivers/cuda/native_executable.c:99: INTERNALï¼›CUDA driver error 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION' (222):the provided PTX was compiled with an unsupported toolchain.;while invoking native function hal.executable.create; while calling import;

### 2. Mismatched IREE and CUDA

For **CUDA 11**, the stable IREE is:
```sh
!pip install iree-compiler==20230524.529 iree-runtime==20230524.529 
```

For **CUDA 12**, the compatible IREE is:
```sh
!pip install iree-compiler==20230815.614 iree-runtime==20230815.614
```

However, newer IREE like v20230815.614 has problems for performance bechmark (iree-benchmark-module), the resolution is:

1) Find if iree-benchmark-module available in _runtime_libs folder
```sh
ls /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/
```
2) Copy it to the runtime folder
```sh
cp /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/iree-benchmark-module /opt/conda/lib/python3.10/site-packages/iree/runtime/
```

### Torch-MLIR, IREE-TF, ONNX-MLIR
Torch-MLIR is bunded with torch (dev), to compile torch models using Torch-MLIR, you need to download both torch-mlir and torch package from their release website, for example https://github.com/llvm/torch-mlir/releases/tag/snapshot-20230525.849 The installed torch-mlir also need to be compatible with IREE. Torch-MLIR requires torchvision 0.16.0
```sh
!pip install torchvision==0.16.0 --no-dependencies
```

IREE-TF and some Tensorflow models has the following dependencies
```sh
!pip uninstall tensorflow -y
!pip install tensorflow-cpu==2.13.0
!pip install tensorflow-addons==0.21.0
!pip install validators
!pip install scipy
!pip install opencv-python
!apt install libgl1 -y
```

If you encouter problems for compiling models using IREE-TF, you may lower the IREE TF tools:
```sh
!pip install iree-tools-tf==20230524.529  iree-tools-tflite==20230524.529
```

ONNX-MLIR on GPU? No official support yet.





